{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SuperMario.ipynb","provenance":[],"collapsed_sections":["9u_bVI_99Rfv","B9mN3hsv9Hl2","APhMTaPP9Kxl"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"k4n_BaeJzDsS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682676628,"user_tz":-330,"elapsed":18232,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["%%capture\n","!pip install gym_super_mario_bros"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zpgt8My59Q0t","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NER4VNsJ9Vgo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682730528,"user_tz":-330,"elapsed":1564,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["ROOT = \"/content/drive/My Drive/Practice/RL/DQN\""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLAOgs3gOPy8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682735848,"user_tz":-330,"elapsed":6874,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["import os\n","import cv2\n","import gym\n","import collections\n","\n","import numpy as np\n","import torch as T\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","from tqdm.notebook import tqdm\n","\n","import gym_super_mario_bros\n","from nes_py.wrappers import JoypadSpace\n","from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n","\n","%matplotlib inline"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZDPJ0g-e6fh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1597682735855,"user_tz":-330,"elapsed":6860,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}},"outputId":"1cfcf176-cce7-421d-f601-46afa2a54633"},"source":["T.cuda.get_device_name()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla P100-PCIE-16GB'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"9u_bVI_99Rfv","colab_type":"text"},"source":["## **Wrappers**"]},{"cell_type":"code","metadata":{"id":"L5aHWPN-OEAJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682735859,"user_tz":-330,"elapsed":6859,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["# PREPROCESS EACH FRAME\n","class PreprocessFrames(gym.ObservationWrapper):\n","    \"\"\"\n","    PREPROCESSES EACH FRAME (input = (rows, columns, 3)) [0-255]\n","    1. resize image                             -   (new_rows, new_columns, 1)       [0-255]\n","    2. convert to nparray                       -   array(new_rows, new_columns, )  [0-255]\n","    3. move axis(reshape)                       -   array(1, new_rows, new_columns)  [0-255]\n","    3. scale values from 0-1                    -   array(1, new_rows, new_columns)  [0.0-1.0]\n","    \"\"\"\n","    def __init__(self, env, new_observation_shape):\n","        super().__init__(env)\n","        self.new_observation_shape = new_observation_shape\n","        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=self.new_observation_shape, dtype=np.float32)\n","    \n","    def observation(self, observation):\n","        temp_frame = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n","        temp_frame = cv2.resize(temp_frame, self.new_observation_shape[1:], interpolation=cv2.INTER_AREA)\n","        new_observation = np.array(temp_frame, dtype=np.float32).reshape(self.new_observation_shape)\n","        new_observation = new_observation / 255.0 \n","        return new_observation\n","\n","# TO BE CALLED ON EACH SINGLE IMAGE (AFTER PREPROCESS)\n","class CustomStep(gym.Wrapper):\n","    \"\"\"\n","    OVERRIDES step() & reset()\n","    1. repeats same action in 'n' skipped frames to compute faster.\n","    2. takes maximum of 2 frames.\n","    \"\"\"\n","    def __init__(self, env, frame_skip):\n","        super().__init__(env)\n","        self.frame_skip = frame_skip\n","        self.observation_shape = env.observation_space.shape\n","        self.observation_buffer = np.zeros_like((2, self.observation_shape))\n","\n","    def reset(self):\n","        observation = self.env.reset()\n","        self.observation_buffer = np.zeros_like((2, self.observation_shape))\n","        self.observation_buffer[0] = observation\n","        return observation\n","\n","    # RETURN FRAME_SKIPPED FRAMES\n","    def step(self, action):\n","        total_reward = 0.0\n","        done = False\n","\n","        for frame in range(self.frame_skip):\n","            observation, reward, done, info = self.env.step(action)\n","            total_reward += reward\n","\n","            idx = frame % 2\n","            self.observation_buffer[idx] = observation\n","\n","            if done: break\n","\n","        observation_max = np.maximum(self.observation_buffer[0], self.observation_buffer[1])\n","        return observation_max, total_reward, done, info\n","\n","\n","# STACK OBSERVATIONS\n","class StackFrames(gym.ObservationWrapper):\n","    \"\"\"\n","    STACKS stack_size FRAMES TOGETHER AND RETURNS AS THE 'observation'\n","    1. on reset() returns first 'observation' STACKED 'stack_size' times\n","    2. observation() returns current 'observation' STACKED with 'stack_size-1' previous 'observation'\n","    \"\"\"\n","    def __init__(self, env, stack_size):\n","        super().__init__(env)\n","        self.observation_space = gym.spaces.Box(\n","                                    env.observation_space.low.repeat(stack_size, axis=0),\n","                                    env.observation_space.high.repeat(stack_size, axis=0)\n","                                 )\n","        self.stack = collections.deque(maxlen=stack_size)\n","\n","    def reset(self):\n","        self.stack.clear()\n","        observation = self.env.reset()\n","        for _ in range(self.stack.maxlen):\n","            self.stack.append(observation)\n","        observation = np.array(self.stack).reshape(self.observation_space.shape)\n","        return observation\n","        \n","    def observation(self, observation):\n","        self.stack.append(observation)\n","        observation = np.array(self.stack).reshape(self.observation_space.shape)\n","        return observation\n","\n","class CustomReward(gym.Wrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","        self._current_score = 0\n","\n","    def step(self, action):\n","        observation, reward, done, info = self.env.step(action)\n","        reward += (info['score'] - self._current_score) / 40.0\n","        self._current_score = info['score']\n","        if done:\n","            if info['flag_get']:\n","                reward += 350.0\n","            else:\n","                reward -= 50.0\n","        return observation, reward / 10.0, done, info"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9YVrXJ0A0nd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682735861,"user_tz":-330,"elapsed":6856,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["# TIE EVERYTHING TOGETHER\n","def make_env(env_name, new_observation_shape=(1,96,96), stack_size=4, frame_skip=4):\n","    env = gym.make(env_name)\n","    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n","    env = PreprocessFrames(env, new_observation_shape=new_observation_shape)\n","    env = CustomStep(env, frame_skip=4)\n","    env = StackFrames(env, stack_size=stack_size)\n","    env = CustomReward(env)\n","    return env"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4SkkGZU9Dd_","colab_type":"text"},"source":["## **ReplayBuffer**"]},{"cell_type":"code","metadata":{"id":"-SvHE9YKo0r-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682735863,"user_tz":-330,"elapsed":6852,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["class ReplayBuffer:\n","    def __init__(self, mem_size, observation_shape, n_actions, alpha):\n","        self.mem_size = mem_size\n","        self.mem_counter = 0\n","        self.ALPHA = alpha\n","        # DATA\n","        self.states = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n","        self.actions = np.zeros(mem_size, dtype=np.int64)\n","        self.rewards = np.zeros(mem_size, dtype=np.int64)\n","        self.states_ = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n","        self.terminals = np.zeros(mem_size, dtype=bool)\n","        self.priorities = np.zeros(mem_size, dtype=np.float32)\n","\n","    # STORE TRANSITIONS IN BUFFER\n","    def store_transition(self, state, action, reward, state_, done):\n","        index = self.mem_counter % self.mem_size\n","        self.states[index] = state\n","        self.actions[index] = action\n","        self.rewards[index] = reward\n","        self.states_[index] = state_\n","        self.terminals[index] = done    # 1 if 'done' else 0\n","        self.priorities[index] = self.priorities.max() if (self.mem_counter>0) else 1.0\n","        self.mem_counter += 1\n","    \n","    # UPDATE PRIORITIES LIST\n","    def update_priotities(self, indices, errors, offset):\n","        priorities = abs(errors) + offset\n","        self.priorities[indices] = priorities\n","\n","    # UNIFORMLY SAMPLES 'BUFFER' AND RETURNS A 'BATCH' OF batch_size\n","    def sample_batch(self, batch_size, beta):\n","        max_index = min(self.mem_counter, self.mem_size) \n","        priorities = self.priorities[:max_index]\n","        probabilities = (priorities ** self.ALPHA) / ((priorities ** self.ALPHA).sum())  # Pr = pi^a/P^a\n","        batch_indices = np.random.choice(max_index, batch_size, p=probabilities)\n","\n","        importance = (max_index * probabilities[batch_indices]) ** (-beta)               # (1/N * 1/Pr)^b\n","        importance = importance / importance.max()\n","        importance = np.array(importance, dtype=np.float32)\n","\n","        states = self.states[batch_indices]\n","        actions = self.actions[batch_indices]\n","        rewards = self.rewards[batch_indices]\n","        states_ = self.states_[batch_indices]\n","        terminals = self.terminals[batch_indices]\n","        return (batch_indices, states, actions, rewards, states_, terminals, importance)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9mN3hsv9Hl2","colab_type":"text"},"source":["## **Network**"]},{"cell_type":"code","metadata":{"id":"N4-yaJCQ0Kci","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682735865,"user_tz":-330,"elapsed":6848,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["class DuelingDeepQNetwork(nn.Module):\n","    def __init__(self, lr, observation_shape, n_actions, model_name, model_dir):\n","        super().__init__()\n","        self.model_dir = model_dir\n","        self.model_file = os.path.join(self.model_dir, model_name)\n","        # CNN\n","        self.conv1 = nn.Conv2d(observation_shape[0], 32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","        # CNN -> ANN\n","        fc_input_dims = self.caculate_conv_output_dims(observation_shape)\n","        # ANN\n","        self.fc1 = nn.Linear(fc_input_dims, 512)\n","        # DUELING\n","        self.V = nn.Linear(512, 1)\n","        self.A = nn.Linear(512, n_actions)\n","        # UTILS\n","        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n","        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n","        self.loss = nn.MSELoss() # CUSTOM LOSS IN AGENT\n","        self.to(self.device)\n","    \n","    def forward(self, state):\n","        t = F.relu(self.conv1(state))\n","        t = F.relu(self.conv2(t))\n","        t = F.relu(self.conv3(t))\n","        t = F.relu(self.fc1(t.reshape(t.shape[0], -1)))\n","        V = self.V(t)\n","        A = self.A(t)\n","        return V,A\n","\n","    def caculate_conv_output_dims(self, observation_shape):\n","        dims = T.zeros((1, *observation_shape))\n","        dims = self.conv1(dims)\n","        dims = self.conv2(dims)\n","        dims = self.conv3(dims)\n","        return int(np.prod(dims.shape))\n","\n","    def save_model(self):\n","        print(\"[INFO] Saving model\")\n","        checkpoint = {\n","            'model_state_dict': self.state_dict(),\n","            'optimizer_state_dict' : self.optimizer.state_dict()\n","        }\n","        T.save(checkpoint, self.model_file)\n","    \n","    def load_model(self, cpu=False):\n","        print(\"[INFO] Loading model\")\n","        \n","        map_location = T.device('cpu') if (cpu) else None\n","        \n","        checkpoint = T.load(self.model_file, map_location=map_location)\n","        self.load_state_dict(checkpoint['model_state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APhMTaPP9Kxl","colab_type":"text"},"source":["## **Agent**"]},{"cell_type":"code","metadata":{"id":"Jf105mZ2pb8I","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682735867,"user_tz":-330,"elapsed":6846,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["class DuelingDDQNAgent:\n","    def __init__(self, observation_shape, n_actions, lr, gamma, epsilon, epsilon_min, epsilon_decay, beta, beta_max, beta_increment,\n","                 mem_size, mem_alpha, batch_size, Q_TARGET_replace_interval, initial_exploration_steps, algo_name, env_name, model_dir):\n","        self.observation_shape = observation_shape\n","        self.n_actions = n_actions\n","        self.LR = lr\n","        self.GAMMA = gamma\n","        self.EPSILON = epsilon\n","        self.epsilon_min = epsilon_min\n","        self.epsilon_decay = epsilon_decay\n","        self.BETA = beta\n","        self.beta_max = beta_max\n","        self.beta_increment = beta_increment\n","\n","        # MEM PARAMS\n","        self.mem_size = mem_size\n","        self.batch_size = batch_size\n","        self.memory = ReplayBuffer(mem_size, observation_shape, n_actions, mem_alpha)\n","\n","        # MODEL PARAMS\n","        self.initial_exploration_steps = initial_exploration_steps\n","        self.exploration_counter = 0\n","        self.learn_counter = 0 # TO UPDATE TARGET NETWORK\n","        self.algo_name = algo_name\n","        self.env_name = env_name\n","        self.model_dir = model_dir\n","        self.Q_TARGET_replace_interval = Q_TARGET_replace_interval\n","        # Q1\n","        self.Q_STEP = DuelingDeepQNetwork(lr, observation_shape, n_actions,\n","                              model_name = env_name+'_'+algo_name+'_Q_STEP',\n","                              model_dir = model_dir)\n","        # Q2\n","        self.Q_TARGET = DuelingDeepQNetwork(lr, observation_shape, n_actions,\n","                              model_name = env_name+'_'+algo_name+'_Q_TARGET',\n","                              model_dir = model_dir)\n","\n","    # e-GREEDY POLICY\n","    def get_action(self, observation, greedy=False):\n","        if ( (np.random.uniform() >= self.EPSILON) or greedy):\n","            observation = T.tensor(observation, dtype=T.float32).to(self.Q_STEP.device)\n","            state = T.unsqueeze(observation, 0)\n","            _,A = self.Q_STEP(state)\n","            action = T.argmax(A).item()\n","        else:\n","            action = env.action_space.sample()\n","        return action\n","\n","    def learn(self):\n","        if (self.exploration_counter < self.initial_exploration_steps): return # return if not explored enough\n","        if (self.memory.mem_counter < self.batch_size): return # return if insufficient samples present\n","        # RESET TARGET NETWORK (every 1000 steps)\n","        self.update_Q_TARGET()\n","\n","        batch_indices, states, actions, rewards, states_, terminals, importance = self.sample_batch()\n","        # PREDICT Q1(s,a)\n","        v1,a1 = self.Q_STEP(states)\n","        q1 = v1 + (a1 - a1.mean(dim=1, keepdim=True)) # q - batch_size * n_actions\n","        indices = np.arange(len(actions))\n","        q1_preds = q1[indices,actions]\n","\n","        # GET V1,A2(s_,A) and V2,A2(s_,A)\n","        v1_, a1_ = self.Q_STEP(states_)\n","        v2_, a2_ = self.Q_TARGET(states_)\n","        # GET Q1(s_,A) and Q2(s_,A)\n","        q1_ = v1_ + (a1_ - a1_.mean(dim=1, keepdim=True))\n","        q2_ = v2_ + (a2_ - a2_.mean(dim=1, keepdim=True))\n","        # argmax(Q1(s_,A)) - (max)a_\n","        # Q2(s_, (max)a_) - TARGETS\n","        a_ = T.argmax(q1_, dim=1)\n","        indices = np.arange(len(a_))\n","        q2_next = q2_[indices, a_]\n","        q2_next[terminals] = 0.0                      # Q2(s_) = 0 where terminal=1\n","        q2_targets = rewards + (self.GAMMA * q2_next)\n","\n","        # CALC LOSS & BACKPROP\n","        errors = (q1_preds - q2_targets)\n","        loss = ((errors ** 2) * importance)\n","        loss = loss.mean()\n","\n","        self.Q_STEP.optimizer.zero_grad()\n","        loss.backward()\n","        self.Q_STEP.optimizer.step()\n","\n","        self.learn_counter += 1\n","        self.decay_epsilon()\n","        self.increment_beta()\n","        self.memory.update_priotities(batch_indices, errors.cpu().detach().numpy(), offset=0.1)\n","\n","    def update_Q_TARGET(self):\n","        if ((self.learn_counter % self.Q_TARGET_replace_interval) == 0):\n","            self.Q_TARGET.load_state_dict(self.Q_STEP.state_dict())\n","    \n","    def decay_epsilon(self):\n","        if (self.EPSILON > self.epsilon_min):\n","            self.EPSILON -= self.epsilon_decay\n","        else:\n","            self.EPSILON = self.epsilon_min\n","\n","    def increment_beta(self):\n","        if(self.BETA < self.beta_max):\n","            self.BETA += self.beta_increment\n","        else:\n","            self.BETA = self.beta_max\n","    \n","    def store_transition(self, state, action, reward, state_, done):\n","        self.memory.store_transition(state, action, reward, state_, done)\n","\n","    def sample_batch(self):\n","        batch_indices, states, actions, rewards, states_, terminals, importance = self.memory.sample_batch(self.batch_size, self.BETA)\n","        states = T.tensor(states).to(self.Q_STEP.device)\n","        actions = T.tensor(actions).to(self.Q_STEP.device)\n","        rewards = T.tensor(rewards).to(self.Q_STEP.device)\n","        states_ = T.tensor(states_).to(self.Q_STEP.device)\n","        terminals = T.tensor(terminals).to(self.Q_STEP.device)\n","        importance = T.tensor(importance).to(self.Q_STEP.device)\n","        return batch_indices, states, actions, rewards, states_, terminals, importance\n","        \n","    def save_models(self):\n","        self.Q_STEP.save_model()\n","        self.Q_TARGET.save_model()\n","    \n","    def load_models(self, cpu=False):\n","        self.Q_STEP.load_model(cpu)\n","        self.Q_TARGET.load_model(cpu)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g7HQrV3WcH2c","colab_type":"text"},"source":["## **Training**"]},{"cell_type":"code","metadata":{"id":"mj6YZi0aXsDX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682736742,"user_tz":-330,"elapsed":7716,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["env_name = 'SuperMarioBros-1-1-v0'\n","env = make_env(env_name)\n","\n","# SIMPLE_MOVEMENT = [\n","#     ['NOOP'],\n","#     ['right'],\n","#     ['right', 'A'],\n","#     ['right', 'B'],\n","#     ['right', 'A', 'B'],\n","#     ['A'],\n","#     ['left'],\n","# ]"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xxXF9QF3xdG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597682748131,"user_tz":-330,"elapsed":19100,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}}},"source":["agent = DuelingDDQNAgent(observation_shape=env.observation_space.shape,\n","                         n_actions=env.action_space.n,\n","                         lr=1e-4,\n","                         gamma=0.99,\n","                         epsilon=1.0,\n","                         epsilon_min=0.01,\n","                         epsilon_decay=1e-6,\n","                         beta=0.4,\n","                         beta_max=1.0,\n","                         beta_increment=6e-5,\n","                         mem_size=15000,\n","                         mem_alpha=0.6,\n","                         batch_size=32,\n","                         Q_TARGET_replace_interval=1000,\n","                         initial_exploration_steps = 10000,\n","                         algo_name='DuelingDDQN',\n","                         env_name=env_name,\n","                         model_dir=ROOT)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPOBUYlc7eOq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597682779755,"user_tz":-330,"elapsed":858,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}},"outputId":"b7af20f4-a580-4ec3-fdc9-7b5497d6111c"},"source":["## IF NEW TRAIN CYCLE\n","N_EPISODES = 5000\n","\n","best_reward = -np.inf\n","episode_rewards, episode_lengths, episode_epsilons, mean_rewards = [],[],[],[]\n","\n","# ## IF SERIALIZED OBJECT EXISTS\n","# train_metadata = T.load((ROOT+\"/train_metadata.pkl\"))\n","\n","# N_EPISODES = 5000 - train_metadata[\"episode_n\"]\n","# best_reward = train_metadata[\"best_reward\"]\n","# episode_rewards = train_metadata[\"episode_rewards\"]\n","# episode_lengths = train_metadata[\"episode_lengths\"]\n","# episode_epsilons = train_metadata[\"episode_epsilons\"]\n","# mean_rewards = train_metadata[\"mean_rewards\"]\n","\n","# agent.EPSILON = episode_epsilons[len(episode_epsilons)-1]\n","# agent.load_models()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[INFO] Loading model\n","[INFO] Loading model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rp2We9miezCT","colab_type":"code","colab":{}},"source":["for episode_n in tqdm(range(N_EPISODES)):\n","    total_reward, total_moves = 0,0\n","\n","    done = False\n","    observation = env.reset()\n","\n","    while not done:\n","        agent.exploration_counter += 1\n","        # e-GREEDY ACTION\n","        action = agent.get_action(observation)\n","        observation_, reward, done, _ = env.step(action)\n","\n","        total_reward += reward\n","        total_moves += 1\n","\n","        # STORE DATA & LEARN\n","        agent.store_transition(observation, action, reward, observation_, done)\n","        agent.learn()\n","\n","        observation = observation_\n","\n","    episode_rewards.append(total_reward)\n","    episode_lengths.append(total_moves)\n","    episode_epsilons.append(agent.EPSILON)\n","\n","    mean_reward = np.mean(episode_rewards[-100:])\n","    mean_rewards.append(mean_reward)\n","    \n","    if(mean_reward > best_reward):\n","        agent.save_models()\n","        best_reward = mean_reward\n","\n","        train_metadata = {\n","            'episode_n': episode_n,\n","            'best_reward': best_reward,\n","            'mean_rewards': mean_rewards,\n","            'episode_rewards': episode_rewards, \n","            'episode_lengths': episode_lengths,\n","            'episode_epsilons': episode_epsilons\n","        }\n","        T.save(train_metadata, (ROOT+\"/train_metadata.pkl\"))\n","\n","    print(\"ITER: \",episode_n,\"\\tRWD: \",round(total_reward,2),\"\\tM_RWD: \",round(mean_reward,2),\"\\tLEN: \",total_moves,\"\\tEPS: \",round(agent.EPSILON,4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cb8lXFP74Hx1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597673357093,"user_tz":-330,"elapsed":1560,"user":{"displayName":"Nimish Santosh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsAwxKElAXr8GplXnBLHw_yAJ2JTflUac_fPVFCQ=s64","userId":"01747774516062944150"}},"outputId":"462ca322-3ce6-4c53-f017-43790e9ce314"},"source":["# ## MANUAL SAVE\n","# agent.save_models()\n","\n","# train_metadata = {\n","#     'episode_n': len(mean_rewards),\n","#     'best_reward': best_reward,\n","#     'mean_rewards': mean_rewards,\n","#     'episode_rewards': episode_rewards, \n","#     'episode_lengths': episode_lengths,\n","#     'episode_epsilons': episode_epsilons\n","# }\n","# T.save(train_metadata, (ROOT+\"/train_metadata.pkl\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[INFO] Saving model\n","[INFO] Saving model\n"],"name":"stdout"}]}]}